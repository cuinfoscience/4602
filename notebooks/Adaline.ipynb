{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Load this code, you don't have to change anything'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def predict(X, w):\n",
    "    a = X.dot(w) < 0  \n",
    "    a = a.astype(int)  # will be True if less than zero\n",
    "    a[a == 1] = -1     # hence gets the label -1\n",
    "    a[a == 0] = 1      # else gets the label 1 b/c greater than 0\n",
    "    return a\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    return np.sum(predict(X, w) == y)/y.size\n",
    "\n",
    "def loss(weights, features, labels):\n",
    "    return np.sum((labels - features.dot(weights)) ** 2)\n",
    "\n",
    "def grad(weights, features, labels):\n",
    "    a =  -2 * np.multiply(features, (labels - features.dot(weights)))\n",
    "    return np.sum(a, axis=0).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "N = 100\n",
    "X = iris.data[:, :2][0:N]  # we only take the first two features and 100 rows\n",
    "y = iris.target[0:N]       # we only take the first two features and 100 rows\n",
    " \n",
    "N, F = X.shape\n",
    "\n",
    "y = y.reshape(N,1)\n",
    "\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Initialize the weights\n",
    "\n",
    "w = np.random.rand(F).reshape(F,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building intuition for the loss function \n",
    "\n",
    "Describe the loss function for Adeline in your own words. What does the equation say? \n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the existing code\n",
    "\n",
    "Using the slides on Adeline as a guide, please examine the `predict`, `accuracy`, `loss` and `grad` functions above. Please describe in your own words what each function is doing \n",
    "\n",
    "[Type your answer here, describing each function]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Random search\n",
    "\n",
    "In a minute we will optimize Adeline via a gradient-based technique. But first, let's take a minute to understand *why* we even want to do this and *what* we are even doing in the first place. We can also optimize the method via random search. To be clear, this is a bad way to actually optimize the function. But it is a great way to build intution for what is actually happening in gradient descent.\n",
    "\n",
    "Start off by implementing the `get_random_weights` function below. The function should return weights returned at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_weights(NumFeatures=2):\n",
    "    '''\n",
    "    Return a vector of shape NumFeatures X 1, filled at random. Numpy has functions for this\n",
    "    \n",
    "    For this assignment NumFeatures=2\n",
    "    '''\n",
    "    pass  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(iters=100):\n",
    "    '''\n",
    "    Implement this random search function\n",
    "    Pseudocode is provided for you. Random search\n",
    "    just keeps trying different weights at random\n",
    "    and then returns the best weights it has found so far\n",
    "    '''\n",
    "    \n",
    "    # initialize current_w and best_loss\n",
    "    current_w = get_random_weights(2)\n",
    "    best_loss = 10000\n",
    "    \n",
    "    for i in range(iters):\n",
    "        new_w = get_random_weights(2)\n",
    "        new_loss = None # fill the variable new_loss with the value of the loss, using w chosen at random\n",
    "        \n",
    "        # Implement the following... \n",
    "        # If the new_loss is less than the best_loss \n",
    "        # Then replace current_w with new_w and replace best_loss with the new loss\n",
    "        \n",
    "    # return the best weights you found, during your random search\n",
    "    return current_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Optimization: Gradient descent\n",
    "\n",
    "Random search is not a great way to optimize most functions, especially if you know the gradient which always points in the direction of greatest increase of a function. We will optimize Adeline with gradient *descent*, which takes steps in the *opposite* direction of the gradient. Why does that make sense? Hint: you shoule describe the Adeline loss function?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n",
      "1214.2340057436254\n"
     ]
    }
   ],
   "source": [
    "ETA = .0001\n",
    "ITERS = 100\n",
    "\n",
    "# Using the slide on Gradient descent as a guide (along with the pseudocode below)\n",
    "# finish the implementation of gradient descent below\n",
    "\n",
    "for i in range(ITERS):\n",
    "\n",
    "    old_loss = loss(weights=w, features=X, labels=y)\n",
    "\n",
    "    # implement me!\n",
    "    # 1. compute the gradient\n",
    "    # 2. update the weights by taking a small step in the opposite direction of the gradient \n",
    "    \n",
    "    new_loss = loss(weights=w, features=X, labels=y)\n",
    "\n",
    "    print(loss(weights=w, features=X, labels=y))  # this should go down each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss by iteration\n",
    "\n",
    "Modify the gradient descent code to plot the loss at each iteration of the algorithm. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by loss \n",
    "\n",
    "Modify the gradient descent code to use the `accuracy` function to measure the accuracy at each iteration of the algorithm. Plot the relationship between accuracy and loss. What do you observe? Does that make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates\n",
    "\n",
    "Try varing the learning rate eta by increasing or decreasing eta by powers of 10. Make a plot showing the learning rate and the accuracy after 100 iterations. Does the algorithm achieve high accuracy for all eta, or only for some learning rates? Why might this be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus: convert the numpy functions into native Python\n",
    "\n",
    "If you find yourself with extra time, try translating each of the functions at the top of the notebook into native Python functions (i.e. use for loops, instead of numpy operations). If you do this step, please send me a message on Canvas and turn in a file called `extra_credit.py` showing your implementation of one or more of the functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
