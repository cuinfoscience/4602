{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The softmax function\n",
    "\n",
    "\n",
    "The softmax function can be written as follows, where $i$ indexes an instance in a dataset and there are $K$ output classes. \n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})} $\n",
    "\n",
    "In this notebook we will learn about the softmax function by implementing it in Python code. This will also help us learn some very fundamental aspects of implementing machine learning algorithms. Concretely we will do the following: \n",
    "   - First, we will implement the softmax function using vanilla Python code.\n",
    "   - Second, we will experiment with implementing the softmax function using vectorized code, and see that it will be much faster.\n",
    "   - Finally, we will also implement a numerically stable and vectorized version of the softmax. This means that we will code a version of the sofmax function that will be able to handle really big or small inputs (which is common in ML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Thinking through the dimensions of our data\n",
    "\n",
    "Say we have 2 features and 3 classes. What is the dimensionality of $x_i$? What is the dimensionality of our scores vector? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = 3   # let K be the number of classes\n",
    "J = 2   # let J be the number of features\n",
    "\n",
    "W = np.random.rand(K, J)  \n",
    "\n",
    "x_i = None\n",
    "\n",
    "scores = None  # scores should be a 1D vector of raw scores, one for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Plotting the exp function\n",
    "\n",
    "- Plot the exp function from -3 to 3. \n",
    "\n",
    "- What will happen to the range as the domain gets very big or small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "x = np.linspace(-3, 3, 50)\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Naive softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sofmax(scores):\n",
    "    '''\n",
    "    Write a \"naive\" version of softmax that does not use vectorized operations\n",
    "    This will be *way* slower than the numpy version, but that is OK\n",
    "    We are just building intuition before jumping to the vectorized version\n",
    "    When you don't know how to code something in ML, it can be good to start with a \n",
    "    simple version using loops before skipping ahead to the vectorized version\n",
    "    '''\n",
    "    numerator = None\n",
    "    denominator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Vectorized code in general\n",
    "\n",
    "Vectorized operations are much faster. A vectorized operation uses linear algebra packages like numpy to do computation very quickly using highly optimized code. Some implementations may also run vector operations in parallel.\n",
    "\n",
    "Let's experiment with vectorized versions of a simple function summing a list of numbers, before moving on to a vectorized softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def L2norm_no_vector(x):\n",
    "    '''\n",
    "    Your code here should not use vector operations\n",
    "    \n",
    "    input: a vector\n",
    "    output, L2 norm of x, \\sqrt{\\Sigma x_i^2}\n",
    "    '''\n",
    "    return 42\n",
    "        \n",
    "def L2norm_with_vectors(x):\n",
    "    '''\n",
    "    Your code here should use vector operations\n",
    "\n",
    "    input: a vector\n",
    "    output, L2 norm of x, \\sqrt{\\Sigma x_i^2}\n",
    "    '''\n",
    "    return 42\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, let's confirm that both implementations return the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2norm_no_vector([2,3,4]),  L2norm_with_vectors(np.asarray([2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test how long it takes our code to run as the list grows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in range(1, 10000, 1000):\n",
    "    runs = []\n",
    "    for i in range(25): # take an average of 25 runs\n",
    "        start = timer()\n",
    "        L2norm_no_vector(np.random.rand(n))\n",
    "        end = timer()\n",
    "        runs.append(end - start)\n",
    "    mean = np.mean(runs)\n",
    "    results.append({\"N\": n, \"f\": 'L2norm_no_vector', \"time\": mean})\n",
    "    \n",
    "for n in range(1, 10000, 1000):\n",
    "    runs = []\n",
    "    for i in range(25): # take an average of 25 runs\n",
    "        start = timer()\n",
    "        L2norm_with_vectors(np.random.rand(n))\n",
    "        end = timer()\n",
    "        runs.append(end - start)\n",
    "    mean = np.mean(runs)\n",
    "    results.append({\"N\": n, \"f\": 'L2norm_with_vectors', \"time\": mean})\n",
    " \n",
    "results = pd.DataFrame(results)\n",
    "alt.Chart(results).mark_line().encode(\n",
    "    x=\"N\",\n",
    "    y=\"time\",\n",
    "    color=\"f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Vectorized softmax\n",
    "\n",
    "Now we are ready to take our first crack at a vectorized softmax function. Remember the equation for the softmax is as follows:\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(scores):\n",
    "    '''\n",
    "    input: \n",
    "        a scores function, of $K$ real values between -\\inf and \\inf\n",
    "        \n",
    "    output:\n",
    "        a vector of probabilities that sums to 1\n",
    "    '''\n",
    "    return 42\n",
    "\n",
    "softmax([1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax questions\n",
    "\n",
    "1. What happens when the scores are the same?\n",
    "2. What happens when one score is bigger than the others\n",
    "3. Why do you think this function might be called a \"soft\" max?\n",
    "4. What happens if you pass in the scores `[10000, 1]`\n",
    "4. What happens if you pass in the scores `[0, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: vectorized and numerically stable softmax \n",
    "\n",
    "Recall our softmax function\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always divide the numerator and denominator of a fraction by $c$. It will affect the value of the numerator and denominator but not their ratio. Hence the probabilities will remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3/9 == (3/2)/(9/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k}) / e^c}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})/ e^c}$\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k}) exp(-c)}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}}) exp(-c)}$\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k} - c) }{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime} } -c ) } = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.26894142, 0.73105858]), array([0.26894142, 0.73105858]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_c(scores, c):\n",
    "    '''\n",
    "    input: \n",
    "        a scores function, of $K$ real values between -\\inf and \\inf\n",
    "        \n",
    "    output:\n",
    "        a vector of probabilities that sums to 1\n",
    "    '''\n",
    "    return np.exp(scores - c)/np.sum(np.exp(scores - c))\n",
    "\n",
    "# lets verify that all the math above is correct ... always a good idea ... \n",
    "softmax(np.asarray([1,2])),  softmax_c(np.asarray([1,2]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take the log of the softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our equation from above:\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k} - c)}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}}  - c)}$\n",
    "\n",
    "Now let's take the log:\n",
    "\n",
    "$ log p(y_i = k | x_i) = \\text{score}_{i, k} - c  - log {\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}}  - c)}$\n",
    "\n",
    "Note that we can use any $c$ we want in the above equation, so let's set $c$ to the largest item in our scores vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_softmax(scores):\n",
    "    '''\n",
    "    input: \n",
    "        a scores function, of $K$ real values between -\\inf and \\inf\n",
    "        \n",
    "    output:\n",
    "        a vector of log probabilities. When expoentitated, these probabilities should sum to 1\n",
    "    '''\n",
    "    c = np.max(scores) # c = None # set c to the max of scores\n",
    "    logsumexp = np.log(np.sum(np.exp(scores - c)))\n",
    "    return scores - c - logsumexp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests for numerical stability and correctness\n",
    "\n",
    "Test for numerical stability and correctness with the following code. What do you notice is different about your log softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softmax([10000, 1]))\n",
    "print(np.exp(log_softmax([10000, 1])))\n",
    "print(np.exp(log_softmax([5, 5])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
