{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annotator': '1', 'annotation': 'positive'},\n",
       " {'annotator': '2', 'annotation': 'positive'},\n",
       " {'annotator': '3', 'annotation': 'positive'},\n",
       " {'annotator': '4', 'annotation': 'negative'},\n",
       " {'annotator': '5', 'annotation': 'negative'},\n",
       " {'annotator': '6', 'annotation': 'positive'},\n",
       " {'annotator': '7', 'annotation': 'positive'},\n",
       " {'annotator': '8', 'annotation': 'positive'},\n",
       " {'annotator': '9', 'annotation': 'positive'},\n",
       " {'annotator': '10', 'annotation': 'positive'},\n",
       " {'annotator': '11', 'annotation': 'positive'},\n",
       " {'annotator': '12', 'annotation': 'positive'},\n",
       " {'annotator': '13', 'annotation': 'negative'},\n",
       " {'annotator': '14', 'annotation': 'positive'},\n",
       " {'annotator': '15', 'annotation': 'positive'},\n",
       " {'annotator': '16', 'annotation': 'positive'},\n",
       " {'annotator': '17', 'annotation': 'positive'},\n",
       " {'annotator': '18', 'annotation': 'negative'},\n",
       " {'annotator': '19', 'annotation': 'positive'},\n",
       " {'annotator': '20', 'annotation': 'positive'},\n",
       " {'annotator': '21', 'annotation': 'positive'},\n",
       " {'annotator': '22', 'annotation': 'positive'},\n",
       " {'annotator': '23', 'annotation': 'positive'},\n",
       " {'annotator': '24', 'annotation': 'positive'},\n",
       " {'annotator': '25', 'annotation': 'positive'},\n",
       " {'annotator': '26', 'annotation': 'positive'},\n",
       " {'annotator': '27', 'annotation': 'positive'},\n",
       " {'annotator': '28', 'annotation': 'positive'},\n",
       " {'annotator': '29', 'annotation': 'positive'},\n",
       " {'annotator': '30', 'annotation': 'negative'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "all_results = []\n",
    "\n",
    "with open(\"class.csv\", \"r\") as inf:\n",
    "    results = csv.reader(inf)\n",
    "    next(results)\n",
    "    for r in results:\n",
    "        annotator, annotation, review = r\n",
    "        all_results.append({\"annotator\": annotator, \"annotation\": annotation, \"review\": review})\n",
    "        \n",
    "# How many annotators are there? \n",
    "# How many data points are there? \n",
    "\n",
    "annotators = set()\n",
    "\n",
    "from collections import defaultdict\n",
    "review2judgements = defaultdict(list)\n",
    "\n",
    "for result in all_results:\n",
    "    review2judgements[result[\"review\"]].append({\"annotator\": result[\"annotator\"], \"annotation\": result[\"annotation\"]})\n",
    "    annotators.add(result[\"annotator\"])\n",
    "\n",
    "len(annotators), len(all_results)\n",
    "\n",
    "review = list(review2judgements.keys())[2]\n",
    "\n",
    "review2judgements[review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7126436781609196"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pairwise_agreement(results):\n",
    "    '''\n",
    "    Compute the pairwise agreement between raters for the input results\n",
    "    \n",
    "    To compute pairwise agreement compare judgements from all pairs of annotators for a given item\n",
    "    Return the fraction of pairs of annotators who agree\n",
    "    '''\n",
    "    total_judgements = {}\n",
    "    for result in results:\n",
    "        for other_result in results:\n",
    "            if result[\"annotator\"] != other_result[\"annotator\"]:\n",
    "                pair = [result[\"annotator\"], other_result[\"annotator\"]]\n",
    "                pair.sort()\n",
    "                pair = \"-\".join(pair)\n",
    "                total_judgements[pair] = (result[\"annotation\"], other_result[\"annotation\"])\n",
    "    out = total_judgements.values()\n",
    "    agrees = 0\n",
    "    for pair in out:\n",
    "        judgement1, judgement2 = pair \n",
    "        if judgement1 == judgement2:\n",
    "            agrees += 1\n",
    "    return agrees/len(out)\n",
    "\n",
    "review4 = {\"1\": 1, \"2\": 0, \"3\": 1}\n",
    "\n",
    "# do 1 and 2 agree ==> No\n",
    "\n",
    "# do 1 and 3 agree ==> Yes\n",
    "\n",
    "# do 2 and 3 agree ==> No\n",
    "\n",
    "# agreement rate = 1/3 = #agreements / #pairs\n",
    "\n",
    "pairwise_agreement(review2judgements[review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = 0\n",
    "review = \"\"\n",
    "for review in review2judgements:\n",
    "    agreement_rate = pairwise_agreement(review2judgements[review])\n",
    "    if agreement_rate > max_:\n",
    "        max_ = agreement_rate\n",
    "        max_review = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After an unpleasant experience at Lotus Grocery on 17 February 2014, I want to caution customers to be careful when paying with cash at the store.   My order was $3.08.  I gave the glum and cold middle aged female cashier $10.10.  She put the $10 bill in the till and gave me $2.02 change, which would have been correct had I given her a $5 bill.  I told her that I gave her a ten dollar bill, and she repeatedly insisted that I did not. I went to the mean middle aged man who appears to be manager or owner and told him of the problem.  He was not interested in hearing it and it eventually fell into the lap of a short, middle aged woman who also appears to be manager, and is one of the nicer people on the staff. I got nowhere until I asked that a count of the register receipts.  The woman who  appears to be in management suggested that I leave my phone number and that I would be called after the store closes and a count is completed. That was not acceptable to me. I asked that a count be conducted while I waited and she agreed.  As she counted and recounted, I waited patiently.  She ultimately concluded that I was right and gave me my $5.00.  She apologized multiple times; the clerk who would have cheated me out of $5 did not. I will be careful when shopping at this store.  I will try to pay with one dollar bills in the future and will announce how much money I am handing to the cashier. There is a bitter taste in my mouth about Lotus Grocery.  Of all places for this to happen, it would have to be one where there is a language barrier and where most of the employees are dismissive of the customer.  This is not Giant Eagle!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-item analysis\n",
    "\n",
    "- Which review has the highest and lowest pairwise agreement rate? Does this make sense?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agreement rate\n",
    "\n",
    "If two reviewers answered randomly (meaning just picked random annotations) how often would they agree just by chance?\n",
    "\n",
    "[Type your answer here, and explain your reasoning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bar{P}_e$ = .5 = \"change agreement rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9366568144499179"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreements = []\n",
    "for review in review2judgements:\n",
    "    agreement_rate = pairwise_agreement(review2judgements[review])\n",
    "    agreements.append(agreement_rate)\n",
    "\n",
    "import numpy as np\n",
    "pbar = np.mean(np.asarray(agreements))\n",
    "\n",
    "pbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fleiss Kappa\n",
    "\n",
    "[Fleiss kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa) measures the exent to which pairs of reviewers agree, as compared to how much they would agree by chance. \n",
    "\n",
    "- $\\bar{P}_e$ is the rate at which reviewers agree by chance \n",
    "- $\\bar{P}$ is the pairwise agreement rate across all items the dataset\n",
    "    - note: the Wikipedia article uses a slightly different definition of $\\bar{P}$, because it assumes all reviewers review all items, which is not true in our case\n",
    "\n",
    "\n",
    "$\\kappa = \\frac{\\bar{P} - \\bar{P}_e}{1-\\bar{P}_e}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8733136288998358"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = .5\n",
    "kappa = (pbar - pe)/(1 - pe)\n",
    "\n",
    "kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = 1 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the highest possible value of Fleiss Kappa? What is the lowest?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the highest possible value of Fleiss Kappa? What is the lowest?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does the denominator mean? If $\\bar{P}_e$ is high, then is the denominator high or low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is high, do you think the task is well-defined?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is high, do you think the task is well-defined\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is low, do you think the task is well-defined?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think the Fleiss Kappa will be for the Yelp data set? Do you think it will be higher or lower than for the emotions dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Fleiss Kappa for the dataset\n",
    "\n",
    "def kappa(Pe, Pbar):\n",
    "    return 0\n",
    "\n",
    "# Fill out this function "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
