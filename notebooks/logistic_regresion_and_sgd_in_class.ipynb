{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def log_prob(z, y_i):\n",
    "    '''\n",
    "    Returns the log_prob for one point\n",
    "    '''\n",
    "    fz = logistic(z)\n",
    "    return y_i * np.log(fz) + (1 - y_i) * np.log(1 - fz)\n",
    "\n",
    "\n",
    "def neg_log_likelihood(X, w, y):\n",
    "    '''Compute the negative log likelihood'''\n",
    "    L = 0\n",
    "    for _x,_y in zip(X, y):\n",
    "        z = w.dot(_x)\n",
    "        L += log_prob(z=z, y_i=_y)\n",
    "    return -1 * L\n",
    "\n",
    "\n",
    "def fast_logistic(X, w):\n",
    "    '''Compute the logistic function over many data points'''\n",
    "    return 1/(1 + np.exp(-1 * X.dot(w)))\n",
    "\n",
    "\n",
    "def grad(_X, w, _y, lambda_=.5):\n",
    "    '''\n",
    "    Return the gradient\n",
    "    \n",
    "    - https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "    '''\n",
    "    grad = np.zeros_like(w)\n",
    "    \n",
    "    N,D= _X.shape\n",
    "    \n",
    "    b = _X * (fast_logistic(_X, w) - _y).reshape((N, 1))\n",
    "\n",
    "    return np.sum(b, axis=0) + (lambda_ * 2 * w)\n",
    "\n",
    "\n",
    "def squared_l2_norm(w):\n",
    "    '''\n",
    "    Return the L2 norm of the weights, squared. \n",
    "    \n",
    "    Remember that we square the norm of the weights,\n",
    "    to make the math easier when computing the gradients\n",
    "    \n",
    "    $\\sqrt{\\Sigma w_i^2} ^ 2\n",
    "    '''\n",
    "    return np.sqrt(np.sum(np.square(w))) ** 2\n",
    "\n",
    "\n",
    "def grad_descent(_X, _y, eta = .0001, lambda_ = 0, tolerance=1e-4, verbose=True, batch_size=None, iters=None):\n",
    "    '''\n",
    "    Perform gradient descent\n",
    "    '''\n",
    "    w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(1000):\n",
    "        if i > iters and iters is not None:\n",
    "            break\n",
    "        this_ll = neg_log_likelihood(_X, w, _y)\n",
    "        loss = this_ll + lambda_ * squared_l2_norm(w)\n",
    "        losses.append(loss)\n",
    "        if verbose:\n",
    "            print(\"iter: {}, loss: {}, accuracy: {}\".format(i, loss, accuracy(_X, w, _y)))\n",
    "        \n",
    "        if (squared_l2_norm(grad(_X, w, _y, lambda_=lambda_))) < tolerance:\n",
    "            break\n",
    "        \n",
    "        if batch_size is None:\n",
    "            w -= eta * grad(_X, w, _y, lambda_=lambda_)\n",
    "        else:\n",
    "            _N,F = _X.shape\n",
    "            idx = np.random.randint(_N, size=batch_size)\n",
    "            w -= eta * grad(_X[idx], w, _y[idx], lambda_=lambda_)/batch_size\n",
    "        \n",
    "    return w, losses\n",
    "\n",
    "def prediction(X, w, threshold=.5):\n",
    "    '''\n",
    "    - Return a Boolean array of length N.\n",
    "    - The array should be True if the weights dotted with the features for a given instance is greater than .5\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "    return X.dot(w) > threshold\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    '''\n",
    "    Return a value between 0 and 1, showing the fraction of data points which have been classified correctly\n",
    "    '''\n",
    "    return np.mean(prediction(X, w) == y)\n",
    "\n",
    "def init_data(N, dim_):\n",
    "    '''\n",
    "    Initialize data. Note how we generate y below. We know how the data is generated.\n",
    "    '''\n",
    "    w = np.random.uniform(low=-1, high=1, size=dim_)\n",
    "    X = (np.random.rand(dim_ * N) > .5).astype(int)\n",
    "    X = X.reshape(N, dim_)\n",
    "\n",
    "    z_ = X.dot(w) + np.random.uniform(low=-1, high=1, size=X.dot(w).size)\n",
    "\n",
    "    y =  1/(1 + np.exp(-1 * z_)) > .5\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 10000\n",
    "dim_ = 10\n",
    "\n",
    "w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "\n",
    "X, y = init_data(N, dim_)\n",
    "\n",
    "split = int(N/2)\n",
    "\n",
    "X_train = X[0:split]\n",
    "X_test = X[split:]\n",
    "y_train = y[0:split]\n",
    "y_test = y[split:]\n",
    "\n",
    "\n",
    "lambda_ = .1\n",
    "\n",
    "w, losses = grad_descent(X_train, y_train,\n",
    "                        eta=1, tolerance=.0001,\n",
    "                        iters=100, verbose=False,\n",
    "                        lambda_=lambda_, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: normalization\n",
    "- Complete the L2 norm function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does the variable `lambda` do in the code above? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens if you set `lambda` to a huge number? What happens if you set `lambda` to a small number?  What should you see in terms of accuracy and the norm of the weights? Try systematically varying lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: Stochastic gradient descent\n",
    "- Print the loss and vary the batch size:\n",
    "    - How do you think that varying eta will vary the amount of noise in the loss?\n",
    "    - How do you think that varying batch size will vary the amount of noise in the loss?\n",
    "    \n",
    "- Test your answers to the previous two questions by making a plot. Your plot should show the loss each iteration, for different batch sizes. You should try batch sizes of 1, 10 and 100. What do you observe in your plot?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
